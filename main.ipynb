{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d650b0-c1fb-491c-a645-d561c9a60a4b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Toy EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ea1b0-08a2-45af-9aaa-9c87be52ad4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69231ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/full/train'\n",
    "IGNORE = ['.DS_Store']\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for label in os.listdir(TRAIN_PATH):\n",
    "    if label in IGNORE:\n",
    "        continue\n",
    "    full_path = f'{TRAIN_PATH}/{label}'\n",
    "\n",
    "    for filename in os.listdir(full_path):\n",
    "        img = cv2.imread(os.path.join(full_path, filename))\n",
    "        train_images.append(img)\n",
    "        train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57069186-f9bf-4b64-aa93-57d8aaa74727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7cf1c",
   "metadata": {},
   "source": [
    "To display images, I use `np.flip(train_images[0], axis=-1)` to change coloring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8379ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.flip(train_images[100], axis=-1))\n",
    "plt.title(train_labels[100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f3065",
   "metadata": {},
   "source": [
    "See how everything works with grey scale, so we could reduce number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imshow(train_images[100])\n",
    "gray = lambda rgb : np.dot(rgb[... , :3] , [0.299 , 0.587, 0.114])\n",
    "gray = gray(train_images[100])\n",
    "plt.imshow(gray, cmap = plt.get_cmap(name = 'gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2161b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = len(train_images)\n",
    "train_images_combined = np.array(train_images).reshape(samples, 224 * 224 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ce7fe",
   "metadata": {},
   "source": [
    "# Deep Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c26f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='data/full/train/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dcff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=64*54*54, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685cfda-cc8c-416b-a452-3c48ee201901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10830d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch {epoch + 1}, batch {i + 1}, loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18d6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(root='data/full/test/', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        num_total += labels.size(0)\n",
    "        num_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = num_correct / num_total\n",
    "print(f'Test accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef752319-89dc-4968-ba0b-4feb95f1853b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Bad accuracy, because:\n",
    "- Not enough observations\n",
    "- No control of weight initilization procedure\n",
    "- Too many classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165db310-596d-424d-be6e-52fde4ec6973",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Pretrained CNN: ConvNet as fixed feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463f0ca-d982-4814-b686-d1d4ec8ea356",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true,
    "user_expressions": []
   },
   "source": [
    "### Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b1523-645b-496a-bdab-41d54cc8d252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class CNNClass():\n",
    "    def __init__(self, model, transform, params, name,\n",
    "                 path = 'data/full',\n",
    "                 criterion = nn.CrossEntropyLoss(), \n",
    "                 optimizer = None,\n",
    "                 device = 'cuda:0',\n",
    "                 lr = 0.001):\n",
    "        \n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "        \n",
    "        train_data = ImageFolder(f'{path}/train', transform=transform)\n",
    "        valid_data = ImageFolder(f'{path}/valid', transform=transform)\n",
    "        \n",
    "        self.train_loader = DataLoader(train_data, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "        self.valid_loader = DataLoader(valid_data, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "        self.params = params\n",
    "        self.name = name\n",
    "        \n",
    "        if not os.path.exists(f'weights/{self.name}'):\n",
    "            os.mkdir(f'weights/{self.name}')\n",
    "\n",
    "        self.criterion = criterion\n",
    "        if optimizer == None:\n",
    "            self.optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.device = device\n",
    "    \n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.losses = {\n",
    "            'train_loss': list(),\n",
    "            'val_loss': list()\n",
    "               }\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epoch_no_improvement = 0\n",
    "        best_epoch = 1\n",
    "        for epoch in range(self.params['EPOCHS']):\n",
    "            running_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            self.model.train()\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(images)\n",
    "                loss = self.criterion(output, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Completed epoch, calculate validation error\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images, labels in self.valid_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "            val_loss = val_loss / len(self.valid_loader.dataset)\n",
    "\n",
    "            print(f'Epoch {epoch+1} Train loss: {epoch_loss:.3f}. Valid loss: {val_loss:.3f}')\n",
    "\n",
    "            self.losses['train_loss'].append(epoch_loss)\n",
    "            self.losses['val_loss'].append(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch + 1\n",
    "                epoch_no_improvement = 0\n",
    "                state = {'model': self.model.state_dict(), \n",
    "                         'optimizer': self.optimizer.state_dict()}\n",
    "                torch.save(state, f'weights/{self.name}/{self.name}.pth')\n",
    "            else:\n",
    "                epoch_no_improvement += 1\n",
    "                \n",
    "            if epoch_no_improvement == self.params['EARLY_STOP']:\n",
    "                print(f'Training completed! No improvement last {epoch_no_improvement} epoches.' +\n",
    "                      f'\\nBest valid accuracy: {best_val_loss:.2f}' +\n",
    "                      f'\\nBest epoch: {best_epoch}')\n",
    "                self.train_time = \"%s\" % (time.time() - start_time)\n",
    "                break\n",
    "        \n",
    "        self.train_time = \"%s\" % (time.time() - start_time)\n",
    "        self.save_info()\n",
    "        print('-'*10)\n",
    "        print(f'Test Accuracy: {self.test():.2f}%')   \n",
    "                \n",
    "    def test(self, path='data/full/test', debug=False):\n",
    "        test_data = ImageFolder(path, transform=self.transform)\n",
    "        test_loader = DataLoader(test_data, batch_size=self.params['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "\n",
    "        self.model.load_state_dict(torch.load(f'weights/{self.name}/{self.name}.pth', \n",
    "                                              map_location=self.device)['model'])\n",
    "        self.optimizer.load_state_dict(torch.load(f'weights/{self.name}/{self.name}.pth',\n",
    "                                                  map_location=self.device)['optimizer'])\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy\n",
    "    \n",
    "    def save_info(self):\n",
    "        # save time\n",
    "        with open(f'weights/{self.name}/train_time.txt', 'w') as f:\n",
    "            f.write(self.train_time)\n",
    "        \n",
    "        # save all stats\n",
    "        with open(f'summary.csv', 'a') as f:\n",
    "            f.write(f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")},{self.name},{self.train_time},{self.test()}\\n')\n",
    "        \n",
    "        # save logs\n",
    "        epoches = [i + 1 for i in range(len(self.losses['train_loss']))]\n",
    "        with open(f'weights/{self.name}/accuracy_log.csv', 'w') as f:\n",
    "            f.write(\"epoch,train_loss,valid_loss\\n\")\n",
    "            for epoch, train_loss, valid_loss in zip(epoches, \n",
    "                                                     self.losses['train_loss'], \n",
    "                                                     self.losses['val_loss']):\n",
    "                f.write(f\"{epoch},{train_loss:.3f},{valid_loss:.3f}\\n\")\n",
    "                \n",
    "    def get_training_time(self):\n",
    "        with open(f'weights/{self.name}/train_time.txt','r') as f:\n",
    "            for line in f:\n",
    "                print(np.round(float(line) / 60))\n",
    "    \n",
    "    def plot_accuracy(self):\n",
    "        if os.path.exists(f'weights/{self.name}/accuracy_log.csv'):\n",
    "            data = pd.read_csv(f'weights/{self.name}/accuracy_log.csv')\n",
    "            fig, axs = plt.subplots(1, 1)\n",
    "            axs.grid(alpha=0.4)\n",
    "            axs.plot(data['epoch'],\n",
    "                     data['train_loss'], label ='Train Loss', color='black')\n",
    "            axs.plot(data['epoch'],\n",
    "                     data['valid_loss'], label='Valid Loss', color='green')\n",
    "            axs.legend()\n",
    "            axs.set_xlabel('Epoch')\n",
    "            axs.set_ylabel('Loss')\n",
    "            axs.set_title(f'Loss for {self.name} model')\n",
    "            return axs\n",
    "        else:\n",
    "            raise Exception(f'Missing weights/{self.name}/accuracy_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc89c7-d4eb-4ec8-bfd6-8203e6c3a4b9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### AlexNet (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3a672-04e5-4366-8082-0f2e1f10753b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "model_sample = alexnet(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 50,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model1 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='alexnet_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b37425-9e6e-4daa-94e9-065275db6c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791962dd-0289-40fb-8db6-d955feec9993",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### ResNet50 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b621cd-f8aa-4b28-bd1f-cbc3f1e2ce67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 50,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model2 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6204e-6c55-4862-97a9-e440a659bf2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb34b2-29f2-4591-9def-7ff0fc0520d6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### ResNet152 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0014ad-d642-4b00-b5c2-50c4bfe7e011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet152_Weights.DEFAULT\n",
    "model_sample = resnet152(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                            100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model3 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='resnet152_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e739d-ec30-4834-8782-c0daedadde6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea490577-4e8f-456b-8d3a-f4d127e5b633",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Inception V3 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837465d-c40f-4344-9f00-8fd7170090d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "model_sample = inception_v3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                      100)\n",
    "model_sample.aux_logits=False\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 50,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model4 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='inception-v3_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2719a08-0a01-4ca5-864c-0b278ef7c0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model4.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb9a23-fa2c-4721-ac98-37849c248dcf",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### EfficientNet-B3 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026341b-8865-4a5e-9389-b6e33153ec19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model_sample = efficientnet_b3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1536, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 30,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model5 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b3_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c711d-b36f-4c08-beca-e221067f811a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865483ed-b5d4-4aec-af4c-28c6f04552f2",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### EfficientNet-B6 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b1549-cf72-4fb7-af3f-d0f11e4ad1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b6, EfficientNet_B6_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B6_Weights.DEFAULT\n",
    "model_sample = efficientnet_b6(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=2304, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 4}\n",
    "\n",
    "model6 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b6_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ade8c3-031a-4d7f-a2ce-0613f0dd420e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model6.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90f1e3-bf8e-46c5-b555-9e1053c35ecf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### EfficientNetV2-S (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90db42c-c6ac-4895-82b0-d9986ae1deec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_s(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-S_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16cc51-a8dc-45a7-bf08-b95860ea0ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38dd7c-054a-46c4-b39c-ae46416c1e07",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### EfficientNetV2-L (native)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9cc641-5ee5-4fdf-80a2-f46f977ae862",
   "metadata": {},
   "source": [
    "Here, I need to tell what the difference between previous models and this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea20ac-a326-412c-8d96-4487d10cb7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_L_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_l(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model8 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-L_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f855c8-2452-4dfe-a581-f3bf6d9748da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model8.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af56e6a-a475-48fa-bfda-b4d16f0c5888",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### ViT-b-14 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b1656-3e05-47ea-9942-6308b29303af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model_sample = vit_b_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model9 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-b-16_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2eef5-f4a6-49c1-baf8-64f15720f1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model9.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d019b-6155-4298-871a-8d9e0a17d28c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### ViT-l-14 (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e565f2-ce41-4046-96f8-5eaddfdbb7ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_l_16, ViT_L_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_L_16_Weights.DEFAULT\n",
    "model_sample = vit_l_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model10 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-l-16_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c171b-196c-4768-8e0a-ee8d4d79b652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model10.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657292d-3862-479e-b769-c9f47dcea503",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### SuffleNet (x1) (native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b2e36-72d9-4bf6-acf3-94532ac6a197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ShuffleNet_V2_X1_0_Weights.DEFAULT\n",
    "model_sample = shufflenet_v2_x1_0(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 40,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model11 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.0001, momentum=0.9),\n",
    "                  name='shufflenet-v2-x1-0_native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e0cd6-5792-4079-9594-a5c34d5a17d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model11.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a1e5e-ebf6-4022-ac8a-3d005a9fa742",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Conclusion**: good idea to try small models, not large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4b10a-fc31-456e-995d-c0fcfa6b5288",
   "metadata": {
    "toc-hr-collapsed": true,
    "user_expressions": []
   },
   "source": [
    "## Use ResNet to Replace Classifier with Custom NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b54f07-a9f7-44ac-9f3a-0b731a5c1785",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### ResNet: CHANGE 1 (Adam-0.0005-DLReDL-ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed11466-9e48-43ba-a1fe-397bbf1acaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(1024, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_changed_(Adam-0.0005-DLReDL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f2091-b95b-47c4-916e-e681aec0d681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664ca50-1e4b-4e9b-b450-bbd08355d2ca",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### ResNet: CHANGE 2 (Adam-0.0005-DLReDLReDL-ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef73ea-de1d-471e-88eb-e4f0cf8fbf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_changed_(Adam-0.0005-DLReDLReDL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f05a7-e76a-48d6-8ac1-9dd3b7c4a109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54655440-62e6-4250-b908-ae0b3f5b2fbb",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### ResNet: CHANGE 3 (SGD-0.001-DLReDL-ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63c844-d652-4a90-bf18-de9e585d0596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9),\n",
    "                  name='resnet50_changed_classifier_(SGD-0.001-DLReDL)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d250789-edf5-4c5a-ad92-20aef84426ef",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56bcd7-8c7a-4c0e-a324-018d3b440931",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ResNet: CHANGE 4 (Adam-0.0005-DLReDL-ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4f6e4-a2cd-4a5c-8b98-e6c6bc805db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(1024, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_changed_(Adam-0.0005-DLReDL-ELU)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10004c44-62b0-492f-b28a-bb78375c7696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb2e62-826a-42e0-b060-9f0f07218816",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### ResNet: CHANGE 5 (Adam-0.0005-DLReDLReDL-ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724652ac-15cd-4d12-b5cb-398b9b5b519f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(2048, 1500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1500, 750),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(750, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_changed_classifier_(DLReDLReDL)_optimized')\n",
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb039635-0250-43a5-bbf7-ab9fecd84056",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b76de-8b97-41ea-a366-bdb51e8c9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_sample = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Increased number of epochs\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model7 = CNNClass(model=model_sample,\n",
    "                  transform=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.005, momentum=0.9),\n",
    "                  name='resnet50_changed_classifier_(SGD-0.001-DLReDL)')\n",
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c465a-44de-454d-8cb8-58e830cb6b42",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Image augmentation on training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63191e-944e-4d7c-b83e-7d8aa2416c82",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e0289-d52b-4b11-8948-1bd638751a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class CNNClassExtended(CNNClass):\n",
    "    def __init__(self, model, transforms_, params, name,\n",
    "                 path = 'data/full',\n",
    "                 criterion = nn.CrossEntropyLoss(), \n",
    "                 optimizer = None,\n",
    "                 device = 'cuda:0',\n",
    "                 lr = 0.001):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        train_data = ImageFolder(f'{path}/train', transform=transforms_[0])\n",
    "        valid_data = ImageFolder(f'{path}/valid', transform=transforms_[1])\n",
    "        \n",
    "        self.train_loader = DataLoader(train_data, batch_size=params['BATCH_SIZE'], shuffle=True)\n",
    "        self.valid_loader = DataLoader(valid_data, batch_size=params['BATCH_SIZE'], shuffle=True)\n",
    "        self.params = params\n",
    "        self.name = name\n",
    "        \n",
    "        if not os.path.exists(f'weights/{self.name}'):\n",
    "            os.mkdir(f'weights/{self.name}')\n",
    "\n",
    "        self.criterion = criterion\n",
    "        if optimizer == None:\n",
    "            self.optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.device = device\n",
    "    \n",
    "    def test(self, path='data/full/test', transform=None, debug=False):\n",
    "        \n",
    "        if transform == None:\n",
    "            transform = transform_valid = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "        test_data = ImageFolder(path, transform=transform)\n",
    "        test_loader = DataLoader(test_data, batch_size=self.params['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "\n",
    "        self.model.load_state_dict(torch.load(f'weights/{self.name}/{self.name}.pth', \n",
    "                                              map_location=self.device)['model'])\n",
    "        self.optimizer.load_state_dict(torch.load(f'weights/{self.name}/{self.name}.pth',\n",
    "                                                  map_location=self.device)['optimizer'])\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1447da7-48ce-44d0-aff0-7c6d9dc948a3",
   "metadata": {
    "toc-hr-collapsed": true,
    "user_expressions": []
   },
   "source": [
    "#### Hard Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3b122-1a94-40f8-bbd6-b17e0d2e1e12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### AlexNet (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec4ff3-4c60-4c22-81fe-80e60545a613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "model_sample = alexnet(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model1 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_= transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='alexnet_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4872c-13fc-4978-b181-d9661c0a467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825dd93f-7a80-4ebc-85a1-dfad1f3b6174",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ResNet50 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41fa715-83a5-4e8e-ad7d-f32611ce067e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model2 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc522bc5-36f1-4f58-9fb9-b11591ab4777",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f1072-b95c-4963-893d-f01cc2811dc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ResNet152 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e376fa-bb58-4b71-8fef-de2bd7a0cffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet152_Weights.DEFAULT\n",
    "model_sample = resnet152(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                            100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model3 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='resnet152_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c712276-ac7a-4f2b-897c-7da53f6a2999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392a851-d75f-4f74-9cdd-077ea22a4fa0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "###### Inception V3 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bd7ec-0028-4844-95ce-b0eaf7764563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "model_sample = inception_v3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                      100)\n",
    "model_sample.aux_logits=False\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model4 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='inception-v3_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463d335-21b1-47b1-b181-c47661424e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model4.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc1489-7689-44cc-8cdd-32430dd05db3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNet-B3 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d1735-4698-4b0a-879b-86d0da9e594b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model_sample = efficientnet_b3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1536, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model5 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b3_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651a184-08f0-4912-b559-712a91f69c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f1593-e391-4a69-af05-7e90b9bf927c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNet-B6 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba63d8-ada8-48d0-8803-6d718bb9db78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b6, EfficientNet_B6_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B6_Weights.DEFAULT\n",
    "model_sample = efficientnet_b6(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=2304, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 4}\n",
    "\n",
    "model6 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b6_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3183d85a-4945-4b96-aca7-c6462d8c973a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model6.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8538edc-62bf-4bdc-8dd7-0fc01effa9da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNetV2-S (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb7fb3-b1ab-4e10-9479-7cd092af6074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_s(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model7 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-S_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781cf68-df5c-4f08-b7b6-e7c73300e8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef2abf-d423-4803-9da0-6eebb6b50b54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNetV2-L (hard augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab13bdb-b1a1-4fe4-98d1-0574a5455027",
   "metadata": {},
   "source": [
    "Here, I need to tell what the difference between previous models and this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73039713-45b1-4df9-ab32-27b983ec044f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_L_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_l(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model8 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-L_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec9a9f-152a-4a32-a3b8-3f3b54bd8959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model8.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce853989-74db-4b80-af08-5a86bb8f552e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ViT-b-14 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b0145-c043-40ae-a944-fef4d734d5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model_sample = vit_b_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model9 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-b-16_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374b2ba-ba18-4d01-88be-2dbac2611090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model9.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086acc78-1c36-4dc7-b669-a9bb513c6e2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ViT-l-14 (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec6ff4-5dac-4fc3-8798-beab262e5242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_l_16, ViT_L_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_L_16_Weights.DEFAULT\n",
    "model_sample = vit_l_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model10 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-l-16_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02cf01-5f91-401e-a5ee-775e31c6d470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model10.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748256e-1d09-464e-988c-7effd66ce402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### SuffleNet (x1) (hard augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0cd5d-38c0-479e-8a09-25c9a7aee27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ShuffleNet_V2_X1_0_Weights.DEFAULT\n",
    "model_sample = shufflenet_v2_x1_0(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomEqualize(p=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomPerspective(p=0.3, distortion_scale=0.2),\n",
    "    transforms.RandomAutocontrast(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model11 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.0001, momentum=0.9),\n",
    "                  name='shufflenet-v2-x1-0_hard_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c60a9-5fd3-47c4-ac8c-194a0545a8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model11.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c9505-d52b-4777-9223-560398c6cbeb",
   "metadata": {
    "toc-hr-collapsed": true,
    "user_expressions": []
   },
   "source": [
    "#### Sorf Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bd0b8-d179-4f20-9912-1f1cd8b1516c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### AlexNet (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2457ff-1043-429d-962a-e9d69b220f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "model_sample = alexnet(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model1 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='alexnet_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc850023-a363-4a2b-a4f5-a7a542a7d06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa23cd0-0b8c-4561-b723-e176df3694f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ResNet50 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45181d-19cf-4b7a-8d5c-ade125618b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model_sample = resnet50(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model2 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='resnet50_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97e8ad-17f3-437c-b091-f9d3103552ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f67226-93b2-412f-a3d3-8695b875b399",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ResNet152 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff85c3-40ec-4459-9f0a-3a967510b50a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ResNet152_Weights.DEFAULT\n",
    "model_sample = resnet152(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                            100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model3 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='resnet152_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9cdeb-2ac7-43bb-89b5-17ec52d79cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578cc4f0-8913-410d-b022-8d61f0ae0585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### Inception V3 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19996cfc-f844-4abf-95d4-7e792db9a11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = Inception_V3_Weights.DEFAULT\n",
    "model_sample = inception_v3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features,\n",
    "                      100)\n",
    "model_sample.aux_logits=False\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model4 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='inception-v3_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb702a3b-97ec-4a3c-9ad2-67506497deda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model4.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087da50-89bb-4b37-9660-57120707e3bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNet-B3 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3991ed-5f98-42c8-893e-026167494caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model_sample = efficientnet_b3(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1536, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 4}\n",
    "\n",
    "model5 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b3_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d79e0-6c68-4472-ba80-164e3c2bbbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model5.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090a930-e20e-40fe-81b7-9089e4b9fbe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNet-B6 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e05f96-a8ad-40a3-b13d-711df0e59241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b6, EfficientNet_B6_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_B6_Weights.DEFAULT\n",
    "model_sample = efficientnet_b6(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=2304, out_features=100)\n",
    ")\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 4}\n",
    "\n",
    "model6 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.Adam(model_sample.parameters(), lr=0.0005),\n",
    "                  name='efficientnet-b6_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce158d-cd12-40ba-a070-200cf66b6d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model6.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d1ee8-410d-4ebd-bccb-92124c5499de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNetV2-S (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e69ae-bc74-4435-ad07-081bd0909cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_S_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_s(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model7 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-S_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caab0c7-9b65-4466-b5b4-a3e09e4e023c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model7.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3f2f8-dfc7-4178-bbee-4980f7b88536",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### EfficientNetV2-L (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181f805-5aed-4c0a-b900-d43f8cde1123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = EfficientNet_V2_L_Weights.DEFAULT\n",
    "model_sample = efficientnet_v2_l(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.classifier[-1] = nn.Linear(model_sample.classifier[-1].in_features,\n",
    "                                        100)\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model8 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  name='efficientnetV2-L_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106cc16-877b-45d1-9bea-3fe4f73705e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model8.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b35217-dcf2-4260-a45f-bc7b6bfe5e08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ViT-b-14 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bfac3-d705-4144-ace8-30127f14a45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model_sample = vit_b_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model9 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-b-16_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66d74c-3a3d-4073-ab00-e06af0ddd964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model9.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafc71e-6aa4-4377-b167-9dca8b55e3cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "##### ViT-l-14 (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30259a67-5bd2-41c1-bf96-b727778aef64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_l_16, ViT_L_16_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_L_16_Weights.DEFAULT\n",
    "model_sample = vit_l_16(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.heads.head = nn.Linear(model_sample.heads.head.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "                'EPOCHS': 100,\n",
    "                'EARLY_STOP': 5}\n",
    "\n",
    "model10 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.001, momentum=0.9, weight_decay=0.03),\n",
    "                  name='vit-l-16_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6011a7-ac64-4b30-b0f0-b11314614b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model10.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17671026-32cf-4413-8bab-b1371103488d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "##### SuffleNet (x1) (soft augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92a58c-4d29-469a-8263-04bcab9b5c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import shufflenet_v2_x1_0, ShuffleNet_V2_X1_0_Weights\n",
    "\n",
    "device_sample = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ShuffleNet_V2_X1_0_Weights.DEFAULT\n",
    "model_sample = shufflenet_v2_x1_0(weights=weights)\n",
    "\n",
    "for param in model_sample.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_sample.fc = nn.Linear(model_sample.fc.in_features, 100)\n",
    "\n",
    "model_sample.to(device_sample)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_sample = [transform_test, transform_valid]\n",
    "\n",
    "model_params = {'BATCH_SIZE': 32,\n",
    "          'EPOCHS': 100,\n",
    "          'EARLY_STOP': 5}\n",
    "\n",
    "model11 = CNNClassExtended(model=model_sample,\n",
    "                  transforms_=transform_sample,\n",
    "                  params=model_params,\n",
    "                  optimizer = optim.SGD(model_sample.parameters(), lr=0.0001, momentum=0.9),\n",
    "                  name='shufflenet-v2-x1-0_soft_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891734e0-88ae-4900-8e04-2aedc773e4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model11.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c905e8-698c-448b-992f-c07f8af69674",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Some Code to Fetch Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bce13a-dcbd-4f50-8324-c8ec0c61cb9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "test_image = Image.open('data/full/valid/bmx/4.jpg')\n",
    "\n",
    "model.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transformed_image = transform(test_image)\n",
    "batch_image = torch.unsqueeze(transformed_image, 0)\n",
    "output = model(batch_image)\n",
    "\n",
    "idx = np.argmax(torch.nn.functional.softmax(output, dim=1)[0].detach().numpy())\n",
    "train_loader.dataset.classes[idx]\n",
    "\n",
    "model.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transformed_image = transform(test_image)\n",
    "batch_image = torch.unsqueeze(transformed_image, 0)\n",
    "output = model(batch_image)\n",
    "\n",
    "idx = np.argmax(torch.nn.functional.softmax(output, dim=1)[0].detach().numpy())\n",
    "train_loader.dataset.classes[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
